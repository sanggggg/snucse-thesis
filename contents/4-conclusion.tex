\chapter{결론}\label{chap:conclusion}

본 연구는 AI 코딩 에이전트와의 상호작용 데이터를 다면적으로 수집하는 RetroChat 툴킷과, 수집된 세션으로부터 사용자 숙련도 루브릭을 자동 학습·적용하는 LLM-as-a-judge 파이프라인을 구현하였다. Trainer는 상위 퍼센타일 세션만을 선별해 비동기 LLM 호출로 루브릭을 추출하고, LLM 요약 또는 의미론적 클러스터링을 통해 대표 루브릭을 생성한다. Evaluator는 동일 루브릭을 이용해 신규 세션을 1\textasciitilde5점 척도로 점수화하고, 결과를 JSON으로 저장함으로써 분석 자동화를 달성하였다.

실험적으로는 10명의 Vibe Coders로부터 확보한 1{,}000개 Claude Code 세션을 기반으로, 토큰 효율과 사용자 턴 효율이라는 두 가지 목적 점수에 대해 각각 루브릭을 생성하였다. 토큰 효율 기준으로는 상위 15\%에 해당하는 70개 세션에서 277개 루브릭을 추출하였으며, LLM 기반 요약 방식으로 3개의 대표 루브릭을, UMAP + HDBSCAN 기반 군집화 방식으로는 98개 클러스터 중 상위 5개를 선택하여 5개의 대표 루브릭을 도출하였다. 사용자 턴 효율 기준으로는 70개 세션에서 274개 루브릭을 추출하였으며, LLM 방식으로 3개, HDBSCAN 방식으로 5개의 대표 루브릭을 생성하였다. 52개 세션으로 구성된 검증 데이터셋에 대해 Kendall의 tau 상관계수를 측정한 결과, 토큰 효율 기준에서는 HDBSCAN 방식이 tau=0.1174 (p=0.271), LLM 방식이 tau=0.0991 (p=0.330)을, 사용자 턴 효율 기준에서는 LLM 방식이 tau=0.0456 (p=0.355), HDBSCAN 방식이 tau=0.0353 (p=0.448)을 기록하였다. 모든 경우에서 p값이 0.05를 초과하여 통계적으로 유의미한 상관관계가 발견되지 않았으나, 토큰 효율 기준의 HDBSCAN 방식이 상대적으로 가장 높은 상관 경향을 보였다.

그러나 데이터의 정답 라벨이 순증 LOC 대비 토큰/턴 수라는 단순 정량 지표에 치우쳐 있고, 모델이 참조할 추가 맥락(예: 빌드 성공 여부, 테스트 통과율, 코드 리뷰 피드백)이 부족하다는 점이 한계로 남는다. 또한 질적 루브릭과 정량 목적 점수 간의 본질적 차이로 인해 두 척도가 서로 다른 차원을 측정하며, 검증 표본 크기(52개)가 제한적이어서 통계적 검정력이 낮아진 것으로 분석된다.

이러한 한계에도 불구하고 본 연구의 루브릭 기반 평가 방식은 중요한 시사점을 제공한다. 기존의 단순 정량 지표(토큰 사용량, 대화 턴 수 등)는 사용자에게 "얼마나" 효율적이었는지만 알려줄 뿐, "어떻게" 개선해야 하는지에 대한 구체적 방향을 제시하지 못한다. 반면 본 연구의 루브릭은 \textbf{초기 요청의 명확성}, \textbf{반복적 피드백의 실행 가능성}, \textbf{작업 위임과 자율성 부여} 등 사용자 행동의 특정 측면을 세분화하여 평가하고, 각 항목별로 1~5점 척도의 점수와 함께 구체적인 개선 근거를 제공한다. 이는 사용자가 자신의 상호작용 패턴에서 어떤 부분이 우수하고 어떤 부분이 미흡한지를 이해할 수 있게 하며, 설명 가능한 피드백(explainable feedback)을 통해 AI 코딩 에이전트 활용 능력의 체계적 개선을 가능하게 한다. 특히 초보 사용자의 경우, 상위 퍼센타일 세션으로부터 학습된 루브릭을 참고하여 효과적인 프롬프팅 전략과 작업 위임 방식을 학습할 수 있으며, 이는 단순한 점수 비교를 넘어 실질적인 역량 향상으로 이어질 수 있다.

이러한 설명 가능한 피드백은 본 연구의 또 다른 핵심 기여인 'RetroChat' GUI 애플리케이션을 통해 사용자에게 전달된다. RetroChat은 데이터 수집 도구를 넘어, 루브릭 기반 평가 결과를 직관적으로 시각화하고 사용자의 성장을 돕는 실용적인 분석 및 회고 플랫폼으로서 기능한다. 사용자는 자신의 과거 채팅 세션을 단순히 다시 읽는 것을 넘어, 각 세션이 어떤 기준으로 평가되었는지, 자신의 어떤 상호작용 방식이 긍정적 혹은 부정적 평가를 받았는지에 대한 상세한 피드백을 GUI를 통해 직접 확인할 수 있다. 이는 복잡한 LLM-as-a-judge 파이프라인의 분석 결과를 최종 사용자가 쉽게 소비하고 자신의 AI 활용 능력을 성찰하는 도구로 사용할 수 있게 만든다는 점에서 큰 의의를 가진다.

향후에는 (1) RetroChat 툴킷을 통한 더 다양한 벤더/언어 세션 확보, (2) 도메인 전문가의 직접 어노테이션을 포함한 다중 레이블 구축, (3) 코드 실행 성공률, 테스트 통과율 등 결과 기반 지표의 통합, (4) 루브릭 가중치 최적화 및 메타-평가(ensemble judges) 도입 등을 통해 보다 신뢰도 높은 사용자 숙련도 평가 지표를 완성하고자 한다.
